# -*- coding: utf-8 -*-
"""Supply Chain Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12U60UvOR6lXh7gAteBL1cGQCdexU6I1I

# Task
You are required to perform Supply Chain Analysis to find data-driven approaches to optimize the supply chain performance and improve customer satisfaction while reducing costs and maximizing profits for all stakeholders involved.

Here is all the data you need:
"supply_chain_data.csv"
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load the supply chain data
df = pd.read_csv('supply_chain_data.csv')

# Select relevant features for content-based filtering
features = ['Lead times', 'Costs', 'Defect rates', 'Shipping times', 'Order quantities']
data = df[features]

# Standardize the features using StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Each row in 'scaled_data' represents an item (e.g., a combination of supplier, product, transportation mode)
item_profiles = pd.DataFrame(scaled_data, index=df.index, columns=features)

# Calculate cosine similarity between item profiles
similarity_matrix = cosine_similarity(item_profiles)

def recommend_hindering_factors(item_index, top_n=5):
    """Recommends hindering factors based on similarity to a given item.

    Args:
        item_index: The index of the item to find recommendations for.
        top_n: The number of recommendations to return.

    Returns:
        A list of indices of the recommended items.
    """

    # Get similarity scores for the given item
    similarity_scores = similarity_matrix[item_index]

    # Sort items by similarity score in descending order
    sorted_indices = similarity_scores.argsort()[::-1]

    # Exclude the given item itself from the recommendations
    recommended_indices = sorted_indices[sorted_indices != item_index][:top_n]

    return recommended_indices

# Example: Get recommendations for item with index 10
recommended_items = recommend_hindering_factors(10)

# Print the recommended items and their features
print("Recommended items:")
for item_index in recommended_items:
    print(f"Item index: {item_index}")
    print(f"Features: {item_profiles.loc[item_index]}")

"""## Data loading

### Subtask:
Load the supply chain data from the provided CSV file into a pandas DataFrame.

**Reasoning**:
Load the data from the provided CSV file into a pandas DataFrame and display the first and last few rows to verify.
"""

import pandas as pd

try:
    df = pd.read_csv('supply_chain_data.csv')
    display(df.head())
    display(df.tail())
except FileNotFoundError:
    print("Error: 'supply_chain_data.csv' not found.")
    df = None  # Set df to None to indicate failure
except pd.errors.EmptyDataError:
    print("Error: 'supply_chain_data.csv' is empty.")
    df = None
except pd.errors.ParserError:
    print("Error: Unable to parse 'supply_chain_data.csv'. Check file format.")
    df = None
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    df = None

"""## Data exploration

### Subtask:
Explore the loaded dataset to understand its characteristics.

**Reasoning**:
Explore the dataset by examining its shape, data types, missing values, and distributions of key variables.  This involves using methods like `.shape`, `.info()`, `.isnull().sum()`, descriptive statistics, and histograms/boxplots.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Examine the shape of the DataFrame
print("Shape of the DataFrame:", df.shape)

# 2. Determine data types and identify potential type conversions
print("\nData Types:")
print(df.dtypes)

# 3. Check for missing values
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
print("\nMissing Values:")
print(missing_values)
print("\nPercentage of Missing Values:")
print(missing_percentage)

# 4. Analyze the distribution of key variables
key_variables = ['Order quantities', 'Lead times', 'Costs']
print("\nDescriptive Statistics for Key Variables:")
print(df[key_variables].describe())

plt.figure(figsize=(15, 5))
for i, col in enumerate(key_variables):
    plt.subplot(1, 3, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()


plt.figure(figsize=(15, 5))
for i, col in enumerate(key_variables):
    plt.subplot(1, 3, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
    plt.ylabel(col)
plt.tight_layout()
plt.show()

# 5. Identify potential outliers
# Outliers are visually identified in the boxplots above.
print("\nPotential Outliers:")
print("Refer to the boxplots above for visualization of potential outliers.")
print("Further analysis and potential handling methods will be performed in the data cleaning stage.")

"""## Data cleaning

### Subtask:
Clean the data by handling outliers and inconsistencies.

**Reasoning**:
Clean the data by handling outliers, inconsistencies, and duplicates as instructed. Winsorize 'Order quantities', 'Lead times', and 'Costs' to limit the influence of outliers. Correct data types and handle inconsistencies in categorical variables. Remove duplicate rows, keeping the first occurrence.
"""

import numpy as np

# Outlier Treatment: Winsorization
for col in ['Order quantities', 'Lead times', 'Costs']:
    # Calculate the 1st and 99th percentiles
    lower_limit = np.percentile(df[col], 1)
    upper_limit = np.percentile(df[col], 99)

    # Winsorize the data
    df[col] = np.clip(df[col], lower_limit, upper_limit)


# Data Type Handling: No explicit type conversion needed as no inconsistencies are apparent.
# Further analysis can be done if specific inconsistencies are found.


# Inconsistency Handling: Check for inconsistencies in categorical variables and standardize values.
for col in ['Product type', 'Customer demographics', 'Inspection results', 'Transportation modes', 'Routes']:
    df[col] = df[col].str.strip().str.lower()  # Remove leading/trailing spaces and convert to lowercase

# Duplicate Removal
df.drop_duplicates(keep='first', inplace=True)

display(df.head())

"""## Data wrangling

### Subtask:
Create new features from existing ones to enrich the dataset for further analysis.

**Reasoning**:
Calculate total cost per order, average delivery time, and order frequency. Then, one-hot encode categorical features and drop the original categorical columns.
"""

# Calculate total cost per order
df['Total Cost per Order'] = df['Shipping costs'] + df['Manufacturing costs'] + df['Costs']

# Calculate average delivery time
df['Average Delivery Time'] = (df['Lead times'] + df['Shipping times'] + df['Lead time'] + df['Manufacturing lead time']) / 4

# Calculate order frequency (assuming 'Customer demographics' identifies unique customers)
order_frequency = df.groupby('Customer demographics')['SKU'].count().reset_index()
order_frequency.rename(columns={'SKU': 'Order Frequency'}, inplace=True)
df = pd.merge(df, order_frequency, on='Customer demographics', how='left')

# One-hot encode categorical features
categorical_cols = ['Product type', 'Customer demographics', 'Inspection results', 'Transportation modes', 'Routes']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

display(df.head())

"""## Data analysis

### Subtask:
Analyze key performance indicators (KPIs) and identify potential bottlenecks in the supply chain.

**Reasoning**:
Analyze KPIs, identify bottlenecks, explore correlations, and document findings.  Calculate KPIs if not already present, examine relationships between supply chain stages, visualize correlations using scatter plots and correlation matrices, and summarize key observations.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Calculate Order Fulfillment Rate (assuming successful orders are those with no defects)
df['Order Fulfillment Rate'] = (df['Defect rates'] == 0).astype(int)

# Analyze KPIs
print("KPI Analysis:")
print(df[['Order Fulfillment Rate', 'Average Delivery Time', 'Order quantities', 'Total Cost per Order']].describe())


# Identify Bottlenecks (correlations between lead times and costs, transportation modes and delays)
print("\nBottleneck Analysis (Correlations):")
print(df[['Lead times', 'Costs', 'Average Delivery Time', 'Order Fulfillment Rate', 'Transportation modes_road', 'Transportation modes_rail', 'Transportation modes_sea']].corr())

plt.figure(figsize=(10, 8))
sns.heatmap(df[['Lead times', 'Costs', 'Average Delivery Time', 'Order Fulfillment Rate', 'Transportation modes_road', 'Transportation modes_rail', 'Transportation modes_sea']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Key Variables')
plt.show()

# Visualize correlations
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Order quantities', y='Average Delivery Time', data=df, hue='Order Fulfillment Rate')
plt.title('Order Quantities vs. Average Delivery Time')
plt.xlabel('Order Quantities')
plt.ylabel('Average Delivery Time')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Average Delivery Time', y='Total Cost per Order', data=df, hue='Order Fulfillment Rate')
plt.title('Average Delivery Time vs. Total Cost per Order')
plt.xlabel('Average Delivery Time')
plt.ylabel('Total Cost per Order')
plt.show()

# Document Findings
print("\nSummary of Findings:")
print("1.  KPIs: The descriptive statistics of key KPIs are provided above. The average delivery time, cost, and order quantities can be further examined.")
print("2. Bottlenecks: Potential bottlenecks are identified by examining correlations. Analyze the correlations between variables like lead times, costs, transportation modes, and order fulfillment rates. Further investigation into these relationships can help identify constraints in the supply chain.")
print("3. Correlations: Visualizations of 'Order Quantities vs. Average Delivery Time' and 'Average Delivery Time vs. Total Cost per Order' reveal the correlations between these variables. ")
print("Further Analysis: Investigate the relationship between defect rates, order fulfillment, and other key factors. Explore potential interactions between variables, and segment the data by different customer groups or product types to detect any significant trends or anomalies.")

"""## Data visualization

### Subtask:
Visualize the key findings from the data analysis to gain deeper insights into the supply chain.

**Reasoning**:
Create histograms, box plots, scatter plots, and a heatmap to visualize the key findings from the data analysis.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histograms with KDE
plt.figure(figsize=(16, 10))
for i, col in enumerate(['Order quantities', 'Lead times', 'Costs', 'Average Delivery Time', 'Total Cost per Order']):
    plt.subplot(2, 3, i + 1)
    sns.histplot(df[col], kde=True, color=f"C{i}")
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Box plots
plt.figure(figsize=(16, 10))
for i, col in enumerate(['Order quantities', 'Lead times', 'Costs', 'Average Delivery Time', 'Total Cost per Order']):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(y=df[col], color=f"C{i}")
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

# Scatter plots
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.regplot(x='Order quantities', y='Average Delivery Time', data=df, scatter_kws={'s': 50}, line_kws={"color": "red"}, color='blue')
plt.title('Order Quantities vs. Average Delivery Time')
plt.subplot(1, 2, 2)
sns.regplot(x='Average Delivery Time', y='Total Cost per Order', data=df, scatter_kws={'s': 50}, line_kws={"color": "red"}, color='green')
plt.title('Average Delivery Time vs. Total Cost per Order')
plt.tight_layout()
plt.show()

# Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df[['Lead times', 'Costs', 'Order Fulfillment Rate', 'Transportation modes_road', 'Transportation modes_rail', 'Transportation modes_sea']].corr(), annot=True, cmap='viridis', fmt=".2f")
plt.title('Correlation Matrix of Key Variables')
plt.show()

"""## Feature engineering

### Subtask:
Engineer new features from the existing data to improve model performance.

**Reasoning**:
Engineer new features based on time, lag, interaction, rolling statistics, polynomial features, and domain-specific features. Handle potential data type or missing value issues.
"""

# Lag Features
df['Lagged_Order_Quantities'] = df['Order quantities'].shift(1)
df['Lagged_Lead_Times'] = df['Lead times'].shift(1)

# Interaction Terms
df['Order_Quantity_x_Delivery_Time'] = df['Order quantities'] * df['Average Delivery Time']

# Rolling Statistics
df['Rolling_Mean_Costs'] = df['Costs'].rolling(window=3).mean()
df['Rolling_Std_Lead_Times'] = df['Lead times'].rolling(window=3).std()

# Polynomial Features
df['Order_Quantities_Squared'] = df['Order quantities']**2
df['Lead_Times_Cubed'] = df['Lead times']**3

# Domain-Specific Features
df['Cost_per_Quantity'] = df['Total Cost per Order'] / df['Order quantities']
df['Delivery_Time_Difference'] = df['Average Delivery Time'] - df['Lead time']

# Handling Missing Values (introduced by lag features)
df.fillna(method='bfill', inplace=True)
df.fillna(0, inplace=True) # Final fillna for any remaining missing values
display(df.head())

"""## Data splitting

### Subtask:
Split the data into training, validation, and testing sets.

**Reasoning**:
Split the data into training, validation, and testing sets as instructed.
"""

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df.drop('Order Fulfillment Rate', axis=1)
y = df['Order Fulfillment Rate']

# Split data into temporary training set and combined validation/testing set
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Split combined validation/testing set into separate validation and testing sets
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Verify shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""## Model training

### Subtask:
Train a RandomForestClassifier model to predict the 'Order Fulfillment Rate' using the training data.

**Reasoning**:
Train a RandomForestClassifier model using the training data.
"""

from sklearn.ensemble import RandomForestClassifier

# Initialize the RandomForestClassifier with a random_state for reproducibility
rf_model = RandomForestClassifier(random_state=42)

# Train the model using the training data
rf_model.fit(X_train, y_train)

"""**Reasoning**:
The error indicates that the RandomForestClassifier is trying to convert string values to float, which is causing an issue.  The 'SKU' column is likely causing this error.  Remove the 'SKU' column and any other non-numeric columns from the training data before retraining the model.


"""

from sklearn.ensemble import RandomForestClassifier

# Identify and drop non-numeric columns
non_numeric_cols = ['SKU', 'Shipping carriers', 'Supplier name', 'Location']
X_train = X_train.drop(non_numeric_cols, axis=1)

# Initialize and train the model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

"""## Model optimization

### Subtask:
Optimize the RandomForestClassifier model by tuning its hyperparameters using GridSearchCV.

**Reasoning**:
I need to optimize the RandomForestClassifier model using GridSearchCV.
"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5]
}

# Instantiate GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='accuracy', cv=5)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# Get the best estimator
best_rf_model = grid_search.best_estimator_

"""## Model evaluation

### Subtask:
Evaluate the performance of the best RandomForestClassifier model on the validation and test sets.

**Reasoning**:
Evaluate the best model on validation and test sets, calculate metrics, and visualize confusion matrices.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Drop non-numeric columns from validation and test sets
X_val = X_val.drop(non_numeric_cols, axis=1)
X_test = X_test.drop(non_numeric_cols, axis=1)

# Predict on validation and test sets
y_val_pred = best_rf_model.predict(X_val)
y_test_pred = best_rf_model.predict(X_test)

# Calculate evaluation metrics
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)
    return accuracy, precision, recall, f1, cm

val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(y_val, y_val_pred)
test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(y_test, y_test_pred)

# Print evaluation metrics
print("Validation Set Metrics:")
print(f"Accuracy: {val_accuracy:.4f}")
print(f"Precision: {val_precision:.4f}")
print(f"Recall: {val_recall:.4f}")
print(f"F1-score: {val_f1:.4f}")
print("\nTest Set Metrics:")
print(f"Accuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-score: {test_f1:.4f}")

# Visualize confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(val_cm, annot=True, fmt="d", cmap="Blues")
plt.title("Validation Set Confusion Matrix")
plt.subplot(1, 2, 2)
sns.heatmap(test_cm, annot=True, fmt="d", cmap="Greens")
plt.title("Test Set Confusion Matrix")
plt.tight_layout()
plt.show()

"""## Summary:

### 1. Q&A

No questions were posed in the provided data analysis task.


### 2. Data Analysis Key Findings

* **Data Cleaning:** Outliers in 'Order quantities', 'Lead times', and 'Costs' were winsorized, limiting their influence (clipped values outside the 1st and 99th percentiles).  Categorical features were standardized by removing leading/trailing spaces and converting text to lowercase. Duplicate rows were removed (100 rows initially).
* **Feature Engineering:**  New features were engineered, including 'Total Cost per Order', 'Average Delivery Time', 'Order Frequency', and several lag features, interaction terms, rolling statistics, and polynomial features.  The creation of these new features was intended to improve model performance.
* **Model Training & Optimization:** A RandomForestClassifier model was trained and optimized using GridSearchCV.  The best hyperparameters found were `{'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}` which resulted in a perfect accuracy score of 1.0 during the optimization phase on the training set, suggesting potential overfitting.
* **Model Evaluation Issues:** Model evaluation using validation and test sets revealed a significant problem: the model predicted only one class. This resulted in undefined precision, recall, and F1-scores and misleading confusion matrices.  Despite the accuracy being 1.0, this implies a failure to predict the minority class (defective orders), and the high accuracy score is misleading due to class imbalance.

### 3. Insights or Next Steps

* **Investigate Class Imbalance:** The model's inability to predict the minority class (defective orders), highlighted by the evaluation metrics, strongly suggests a class imbalance problem. Address this imbalance using techniques such as oversampling the minority class, undersampling the majority class, or using cost-sensitive learning.
* **Re-evaluate Feature Engineering:** Given the model's apparent failure to generalize, re-evaluate the engineered features.  Some features might be redundant, irrelevant, or even harmful to the model's performance. Consider feature selection techniques to identify the most informative features.  Also, further explore potential interactions between variables.

"""